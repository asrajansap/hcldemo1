apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: st22-shortdump-serving-template
  annotations:
    scenarios.ai.sap.com/name: "Short dump use case"
    scenarios.ai.sap.com/description: "Short dump analysis"
    executables.ai.sap.com/name: "shortdump-executable"
    executables.ai.sap.com/description: "Short dump use case analysis"
  labels:
    ai.sap.com/version: "4.0"
    scenarios.ai.sap.com/id: "st22Shortdump"

spec:

  inputArtifacts:
    - name: dump-json
      description: "SAP short dump in JSON format"
      type: JSON
      path: /app/data/

  outputArtifacts:
    - name: dump-analysis
      description: "Short dump LLM output"
      type: JSON

  template:
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService

    metadata:
      annotations:
        autoscaling.knative.dev/metric: "rps"
        autoscaling.knative.dev/target: "100"
        autoscaling.knative.dev/targetBurstCapacity: "70"

    spec:
      predictor:
        minReplicas: 0
        maxReplicas: 1
        imagePullSecrets:
          - name: rajdoc-registry
        containers:
          - name: kserve-container
            image: docker.io/s0026942030/shortdump:latest
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            env:
              - name: LOG_LEVEL
                value: "INFO"
              - name: LLM_PROVIDER
                value: "openai"
              - name: OPENAI_MODEL
                value: "gpt-4o-mini"
              - name: STORAGE_DB
                value: "/app/data/st22.db"
            resources:
              limits:
                cpu: "1"
                memory: "1Gi"
